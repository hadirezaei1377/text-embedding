{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Activate virtual env\n",
    "# $ !python3 -m virtualenv .venv\n",
    "# $ !source .venv/bin/activate\n",
    "\n",
    "## Install OpenAI package\n",
    "# $ !pip install openai\n",
    "\n",
    "## Export Gilas.io API key\n",
    "# $ os.environ[\"GILAS_API_KEY\"]='...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()  \n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"GILAS_API_KEY\"),\n",
    "    base_url=\"https://api.gilas.io/v1/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_from_messages(messages, \n",
    "                                 model=\"gpt-3.5-turbo\", \n",
    "                                 temperature=0, \n",
    "                                 max_tokens=500):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def get_embedding(text, dimentions=1531, model=\"text-embedding-3-small\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], dimensions=dimentions, model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# load dataset\n",
    "input_datapath = \"../data/reviews_with_embeddings_30.csv\"\n",
    "df = pd.read_csv(input_datapath, index_col=0, delimiter=\";\")\n",
    "df['embedding'] = df['embedding'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial  # for calculating vector similarities for search\n",
    "\n",
    "# search function\n",
    "def strings_ranked_by_relatedness(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
    "    top_n: int = 10\n",
    ") -> tuple[list[str], list[float]]:\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "    query_embedding = get_embedding(query, dimentions=100)\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"Text\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "    return strings[:top_n], relatednesses[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_context(query):\n",
    "    strings, relatednesses = strings_ranked_by_relatedness(query, df, top_n=5)\n",
    "    strings_above_threshold = []\n",
    "\n",
    "    for string, relatedness in zip(strings, relatednesses):\n",
    "        if relatedness > 0.7:\n",
    "            strings_above_threshold.append(string)\n",
    "\n",
    "    return strings_above_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate answer to user query based on detailed product information\n",
    "\n",
    "delimiter = \"####\"\n",
    "system_message = f\"\"\"\n",
    "You are an intelilgent assistant. \\\n",
    "Use the provided context delimited with {delimiter} to respond to the customer's questions. \\\n",
    "If the answer cannot be found in the context, write \"I could not find an answer.\" \\\n",
    "\"\"\"\n",
    "\n",
    "user_message = f\"\"\"\n",
    "What flavors are in McCann's Instant Irish Oatmeal variety pack?\"\"\"\n",
    "\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content': system_message},   \n",
    "{'role':'user',\n",
    " 'content': user_message},  \n",
    "{'role':'assistant',\n",
    " 'content': f\"\"\"context:\\n\\\n",
    " {delimiter}{generate_context(user_message)}{delimiter}\"\"\"},   \n",
    "]\n",
    "\n",
    "display(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion_from_messages(messages)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"How to train a large language model?\"\n",
    "\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content': system_message},   \n",
    "{'role':'user',\n",
    " 'content': user_message},  \n",
    "{'role':'assistant',\n",
    " 'content': f\"\"\"context:\\n\\\n",
    " {delimiter}{generate_context(user_message)}{delimiter}\"\"\"},   \n",
    "]\n",
    "\n",
    "response = get_completion_from_messages(messages)\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
